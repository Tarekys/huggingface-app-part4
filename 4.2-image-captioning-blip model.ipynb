{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "#### <a id=\"top\"></a>\n",
    "# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>Table of contents</b></div>\n",
    "\n",
    "<div style=\"background-color: rgba(60, 121, 245, 0.03); padding:30px; font-size:15px; font-family: consolas;\">\n",
    "<ul>\n",
    "    <li><a href=\"#1\" target=\"_self\" rel=\" noreferrer nofollow\">1. Introduction to Image Captioning </a> </li>\n",
    "    <li><a href=\"#2\" target=\"_self\" rel=\" noreferrer nofollow\">2. Setting Up Working Environment </a></li>\n",
    "    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">3. Loading the Model and Processor </a></li>\n",
    "    <li><a href=\"#4\" target=\"_self\" rel=\" noreferrer nofollow\">4. Loading & Displaying the Image </a></li>\n",
    "    <li><a href=\"#5\" target=\"_self\" rel=\" noreferrer nofollow\">5. Conditional Image Captioning </a></li>\n",
    "    <li><a href=\"#6\" target=\"_self\" rel=\" noreferrer nofollow\">6. Unconditional Image Captioning </a></li>\n",
    "    <li><a href=\"#7\" target=\"_self\" rel=\" noreferrer nofollow\">7. Trying Another Example with a Better Condition </a></li>\n",
    "\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers\n",
    "# torch\n",
    "# datasets\n",
    "# pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T03:51:15.743183Z",
     "iopub.status.busy": "2025-03-27T03:51:15.742865Z",
     "iopub.status.idle": "2025-03-27T03:51:24.485703Z",
     "shell.execute_reply": "2025-03-27T03:51:24.484663Z",
     "shell.execute_reply.started": "2025-03-27T03:51:15.743157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-27T03:57:42.371Z",
     "iopub.execute_input": "2025-03-27T03:57:22.780083Z",
     "iopub.status.busy": "2025-03-27T03:57:22.779738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import the AutoProcessor class from the transformers library which is a convenient tool for handling preprocessing tasks such as tokenizing text and processing images to prepare them for model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-27T03:57:42.372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 4. Loading & Displaying the Image </b></div>\n",
    "\n",
    "Next, to display the image in a Jupyter Notebook or similar environment, you can use the Image class from the PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:35:11.116306Z",
     "iopub.status.busy": "2024-11-09T14:35:11.115674Z",
     "iopub.status.idle": "2024-11-09T14:35:11.284898Z",
     "shell.execute_reply": "2024-11-09T14:35:11.283472Z",
     "shell.execute_reply.started": "2024-11-09T14:35:11.116259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"/kaggle/input/gaza-images/gaza_under_fire.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this code outside a Jupyter Notebook, simply load the image with **Image.open** will not display it. Using **matplotlib** or another image display library will ensure the image is shown correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:36:00.379209Z",
     "iopub.status.busy": "2024-11-09T14:36:00.378715Z",
     "iopub.status.idle": "2024-11-09T14:36:00.538345Z",
     "shell.execute_reply": "2024-11-09T14:36:00.536562Z",
     "shell.execute_reply.started": "2024-11-09T14:36:00.379166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = \"a photograph of\"\n",
    "inputs = processor(image, text, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a caption for the image using the BLIP model with the processed inputs, you can use the generate method of the model. After generating the output, you can decode it to get the caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:36:15.579336Z",
     "iopub.status.busy": "2024-11-09T14:36:15.578873Z",
     "iopub.status.idle": "2024-11-09T14:36:17.595106Z",
     "shell.execute_reply": "2024-11-09T14:36:17.593849Z",
     "shell.execute_reply.started": "2024-11-09T14:36:15.579295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "out = model.generate(**inputs)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally decode the output using the processor.decode method to get the caption in a readable format. Here's the updated code with the decoding and printing of the generated caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:36:35.647963Z",
     "iopub.status.busy": "2024-11-09T14:36:35.647473Z",
     "iopub.status.idle": "2024-11-09T14:36:35.656175Z",
     "shell.execute_reply": "2024-11-09T14:36:35.654488Z",
     "shell.execute_reply.started": "2024-11-09T14:36:35.647918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the caption is very generic, although it describes the image in general. However, it did not provide a more specific caption describing the real caption which is Israel bombing Gaza. Maybe it needs fine-tuning to provide a more realistic caption. Another possible solution we can also provide more guidance in the guidance text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 6. Unconditional Image Captioning </b></div>\n",
    "\n",
    "\n",
    "Let's try Unconditional Image Captioning. This refers to a type of image captioning task where the model generates captions for an image without any specific guidance or additional input beyond the image itself.\n",
    "In other words, the model generates captions solely based on the visual information present in the image, without being given any explicit prompts or cues about what the caption should focus on or describe.\n",
    "We will follow the same steps as in conditional image captioning but without giving a text argument to the processor method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:37:11.076638Z",
     "iopub.status.busy": "2024-11-09T14:37:11.076148Z",
     "iopub.status.idle": "2024-11-09T14:37:15.161921Z",
     "shell.execute_reply": "2024-11-09T14:37:15.160559Z",
     "shell.execute_reply.started": "2024-11-09T14:37:11.076596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inputs = processor(image,return_tensors=\"pt\")\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the returned caption is almost similar to the previous one we got using conditional image captioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 7. Trying Another Example with a Better Condition </b></div>\n",
    "\n",
    "\n",
    "Let's try with another example in which we will provide better textual guidance to the captioning model. We will use the image below in which a group of Israeli soldiers are abusing Palestinian a child in Jerusalem streets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:37:57.584176Z",
     "iopub.status.busy": "2024-11-09T14:37:57.583145Z",
     "iopub.status.idle": "2024-11-09T14:37:57.666165Z",
     "shell.execute_reply": "2024-11-09T14:37:57.664772Z",
     "shell.execute_reply.started": "2024-11-09T14:37:57.584126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image = Image.open(\"/kaggle/input/gaza-images/israeli_abuse_child.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text guidance we will use is Israeli soldiers and let's observe how this will improve the captioning process and also compare the results with the unconditional image captioning method. We will follow the same process as before. We will start by passing both the image and the text prompt to the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:38:11.920976Z",
     "iopub.status.busy": "2024-11-09T14:38:11.920406Z",
     "iopub.status.idle": "2024-11-09T14:38:11.949415Z",
     "shell.execute_reply": "2024-11-09T14:38:11.947889Z",
     "shell.execute_reply.started": "2024-11-09T14:38:11.920925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = \"Israeli soldiers\"\n",
    "inputs = processor(image, text, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will generate the caption for the image using the BLIP model with the processed inputs using the generate method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:49:17.527088Z",
     "iopub.status.busy": "2024-11-09T14:49:17.526063Z",
     "iopub.status.idle": "2024-11-09T14:49:22.250254Z",
     "shell.execute_reply": "2024-11-09T14:49:22.24833Z",
     "shell.execute_reply.started": "2024-11-09T14:49:17.527033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.generate(**inputs)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will decode the output into the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:49:36.912363Z",
     "iopub.status.busy": "2024-11-09T14:49:36.91186Z",
     "iopub.status.idle": "2024-11-09T14:49:36.920078Z",
     "shell.execute_reply": "2024-11-09T14:49:36.918455Z",
     "shell.execute_reply.started": "2024-11-09T14:49:36.912319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the generated caption is very precise even more than I expected. This was done by easily adding better guidance. To observe the difference let's generate the caption without using a textual condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:49:55.612114Z",
     "iopub.status.busy": "2024-11-09T14:49:55.611628Z",
     "iopub.status.idle": "2024-11-09T14:49:59.542057Z",
     "shell.execute_reply": "2024-11-09T14:49:59.540245Z",
     "shell.execute_reply.started": "2024-11-09T14:49:55.612071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inputs = processor(image,return_tensors=\"pt\")\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the difference between the two captions. The first one is very specific and accurate. While the second one is very general. This shows the importance of good textual guidance."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6046767,
     "sourceId": 9853826,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6978552,
     "sourceId": 11180484,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
